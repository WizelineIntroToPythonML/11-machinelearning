{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "![Course Hero](images/hero.png)\n",
    "\n",
    "## What is Artificial Intelligence and Machine Learning?\n",
    "\n",
    "> Machine Learning is like sex in high school. Everybody wants to do it, lots of people say they are doing it, only a few know how, and a very select few actually do it.\n",
    "\n",
    "## Computing Machinery and Intelligence\n",
    "\n",
    "[![Alan Turing Photo](images/alan_turing.jpg)](https://en.wikipedia.org/wiki/Alan_Turing)\n",
    "\n",
    "In 1950 [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) wrote a paper in Mind (a journal in Oxford University) called [\"Computing Machinery and Intelligence\"](https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence). In it he discusses the subject of Artificial Intelligence.\n",
    "\n",
    "## What is Intelligence?\n",
    "\n",
    "**Computational or Structural Intelligence**: A System that perceives itself and its environment and takes action to maximize its goals.\n",
    "\n",
    "**Executive Intelligence**: The System that decides the goals that should be pursued, controlling and guiding the Computational Intelligence.\n",
    "\n",
    "Recommendation: Read [\"La inteligencia fracasada. Teoría y práctica de la estupidez\"](https://es.wikipedia.org/wiki/La_inteligencia_fracasada._Teor%C3%ADa_y_pr%C3%A1ctica_de_la_estupidez), José Antonio Marina, 2004.\n",
    "\n",
    "[![Book Cover](images/inteligencia_fracasada_book.jpg)](https://es.wikipedia.org/wiki/La_inteligencia_fracasada._Teor%C3%ADa_y_pr%C3%A1ctica_de_la_estupidez)\n",
    "\n",
    "## What is Machine Learning\n",
    "\n",
    "It is a part of Artificial Intelligence. We create a **model** trained with the available data using statistics and numerical methods, so we are able to reach automated conclusions over new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we categorize Machine Learning Algorithms\n",
    "\n",
    "There are four big \"Families\" of Machine Learning Algorithms:\n",
    "\n",
    "- Supervised: In which we have examples where we know the right answer (**labeled data**).\n",
    "  - Regression: We want to obtain a discreet value. A number.\n",
    "  - Classification: We want to obtain a discreet value. A category.\n",
    "- Unsupervised: In which we don´t know the right answer (**unlabeled data**).\n",
    "  - Clustering: Separate the observations into discreet groups.\n",
    "  - Generalization: Find the relevant characteristics.\n",
    "  - Association: Which observations frequently go together.\n",
    "- Ensemble Methods: A Bunch of stupid trees learning to correct errors from each other.\n",
    "  - Stacking: Run several models in parallel, and a last one makes the decision.\n",
    "  - Bagging: We average the result of several models.\n",
    "  - Boosting: Each model learns to correct and interpret the errors of the last one sequentially.\n",
    "- Reinforcement Learning: We decide the next action depending on the response to the previous ones.\n",
    "  - Neural Networks: They try to \"mimic\" how we think Neuron Work.\n",
    "  - Deep Learning: Huge neural networks, made possible by the advent of cheap inference computing power.\n",
    "\n",
    "![Machine Learning Kinds](images/machine_learning.jpeg)\n",
    "\n",
    "![Deep Learning Image](images/neural_networks.png)\n",
    "\n",
    "![Clustering using DBScan](images/clustering_dbscan.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it working? How to evaluate a Machine Learning Model.\n",
    "\n",
    "Because of its nature, there is no way to have complete certainty of the results of a Machine Learning Model. There will always be the possibility of error. Almost always our target is to **minimize** the error.\n",
    "\n",
    "![AUROC Graph](images/roc_auc.png)\n",
    "\n",
    "To know the result of our model training we separate our data in two sets.\n",
    "\n",
    "- Train Data: Is the data we are going to use to train the model. Normally 75% to 80%.\n",
    "- Test Data: Data we are going to use in the trained model to see how it behaves. The rest of the data.\n",
    "\n",
    "![Training Result](images/training_result.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Python - SciPy\n",
    "\n",
    "Besides NumPy and pandas, we use the SciPy package as a base of fundamental algorithms.\n",
    "\n",
    "![SciPy Logo](images/scipy_logo.png)\n",
    "\n",
    "## Machine Learning Libraries\n",
    "\n",
    "There are many of different Machine Learning Libraries, for example:\n",
    "\n",
    "- [TensorFlow](https://www.tensorflow.org)\n",
    "- [Theano](https://pypi.org/project/Theano/)\n",
    "- [Keras](https://keras.io)\n",
    "- [PyTorch](https://pytorch.org)\n",
    "- [Orange3](https://orangedatamining.com)\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "![ML Libraries](images/ml-libraries.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "\n",
    "Simple and efficient tools for predictive data analysis\n",
    "\n",
    "- Accessible to everybody, and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable - BSD license\n",
    "\n",
    "![Scikit-learn logo](images/scikit_learn_logo.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "sns.pairplot(iris, hue='species', height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris dataset flowers](images/iris_dataset.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_width = iris[\"petal_width\"]\n",
    "petal_length = iris[\"petal_length\"]\n",
    "\n",
    "plt.scatter(petal_width, petal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the feature matrix petal_width to make it an array of size [n_samples, n_features]. It can be done as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_width_array = petal_width.to_numpy()[:, np.newaxis]\n",
    "petal_width_array.shape\n",
    "type(petal_width_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(petal_width_array, petal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 2.6)\n",
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)\n",
    "print(yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(petal_width, petal_length)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with K-Nearest Neighbor (KNN) Algorithm\n",
    "\n",
    "The KNN algorithm will attempt to infer a new data point’s class by looking at the classes of the majority of its k-neighbors. For example, if five of a new data point’s neighbors had a class of “Large”, while only two had a class of “Medium”, then the algorithm will predict that the class of the new data point is “Large”.\n",
    "\n",
    "![KNN Visualization](images/knn-visualization.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "penguins = sns.load_dataset('penguins')\n",
    "penguins.sample(5)\n",
    "\n",
    "sns.pairplot(penguins, hue='species', height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Penguins](images/penguins.png)\n",
    "\n",
    "### Classifying with one one feature\n",
    "\n",
    "We are only going to use the length of the bill to predict the penguin species.\n",
    "\n",
    "First we clean the data, and select only the values we will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "\n",
    "X = penguins[['bill_length_mm']]\n",
    "y = penguins['species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate our data into the Train and Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"X Train Size: \", len(X_train), \"; Y Test Size\", len(X_test))\n",
    "print(\"Y Train Size: \", len(y_train), \"; Y Test Size\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a KNeighborsClassifier instance\n",
    "\n",
    "Definition:\n",
    "\n",
    "```python\n",
    "KNeighborsClassifier(\n",
    "    n_neighbors=5,          # The number of neighbors to consider\n",
    "    weights='uniform',      # How to weight distances\n",
    "    algorithm='auto',       # Algorithm to compute the neighbors\n",
    "    leaf_size=30,           # The leaf size to speed up searches\n",
    "    p=2,                    # The power parameter for the Minkowski metric\n",
    "    metric='minkowski',     # The type of distance to use\n",
    "    metric_params=None,     # Keyword arguments for the metric function\n",
    "    n_jobs=None             # How many parallel jobs to run\n",
    ")\n",
    "```\n",
    "\n",
    "We create a model using the train data and Euclidean distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict for just one value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict([[44.2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for all the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify how accurate the model was\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(format(accuracy_score(y_test, predictions),\".2%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with all the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = penguins.select_dtypes(include='number')\n",
    "y = penguins['species']\n",
    "\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = KNeighborsClassifier(\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(format(accuracy_score(y_test, predictions),\".4%\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve even more the model including the categorical data\n",
    "\n",
    "![One Hot Encoding Example](images/one_hot_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also we can \"Min-Max\" the numeric values so all of them are in the same scale, and considered equally for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = penguins.drop(columns = ['species'])\n",
    "y = penguins['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['sex', 'island']),\n",
    "    (MinMaxScaler(), ['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm', 'body_mass_g']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_transformer.get_feature_names_out())\n",
    "\n",
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fixed data we get a better prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(p=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "X_test = column_transformer.transform(X_test)\n",
    "X_test = pd.DataFrame(data=X_test, columns=column_transformer.get_feature_names_out())\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(format(accuracy_score(y_test, predictions),\".4%\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26f3fa5db3d133c1347973118ef626e307094b6a97c1676bb01d816af5f15f40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
